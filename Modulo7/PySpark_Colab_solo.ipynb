{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37039684",
   "metadata": {},
   "source": [
    "\n",
    "# PySpark en Google Colab: instalación y primeros pasos\n",
    "\n",
    "Este cuaderno está diseñado **exclusivamente para Google Colab**.  \n",
    "Incluye instalación de **PySpark**, creación de la **SparkSession**, ejemplos prácticos con **RDDs** y notas específicas para Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee78a9",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Notas rápidas para Colab\n",
    "\n",
    "- Colab reinicia el entorno al desconectarse; si vuelves a abrir el cuaderno, **reinstala** dependencias.\n",
    "- Ejecuta las celdas **en orden** (de arriba hacia abajo).\n",
    "- La interfaz **Spark UI** en `localhost:4040` no es accesible directamente desde Colab; más abajo mostramos cómo obtener la URL (puede no ser navegable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f3dc0c",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Instalación de PySpark\n",
    "\n",
    "Ejecuta esta celda para instalar PySpark en el entorno actual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b2a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instalación en el runtime de Colab\n",
    "!pip -q install pyspark\n",
    "print(\"Instalación completa.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640f9a4",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Verificar Java\n",
    "\n",
    "Colab suele traer Java preinstalado. Verifícalo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ad9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!java -version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24423e",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Crear la `SparkSession` y obtener el `SparkContext`\n",
    "\n",
    "`SparkSession` es el punto de entrada para Spark en Python.  \n",
    "`SparkContext` permite trabajar con **RDDs** directamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93924847",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuración básica para Colab. Puedes ajustar memoria si lo requieres.\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Colab_PySpark_Basico\")\n",
    "         # .config(\"spark.driver.memory\", \"3g\")  # Descomenta si necesitas más memoria\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"Versión de Spark:\", spark.version)\n",
    "print(\"Master:\", sc.master)\n",
    "\n",
    "# URL de la Spark UI (en Colab puede no ser accesible externamente)\n",
    "try:\n",
    "    print(\"Spark UI:\", sc.uiWebUrl)\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb66e0",
   "metadata": {},
   "source": [
    "\n",
    "## 4) RDD desde una lista y *lazy evaluation*\n",
    "\n",
    "Las **transformaciones** no se ejecutan inmediatamente; Spark construye un plan de ejecución que se dispara cuando invocamos una **acción**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b787ad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Datos de ejemplo\n",
    "data = list(range(1, 11))\n",
    "\n",
    "# Crear RDD distribuido\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Definimos transformaciones (todavía NO se ejecutan)\n",
    "pares = rdd.filter(lambda x: x % 2 == 0)\n",
    "doble = pares.map(lambda x: x * 2)\n",
    "\n",
    "# Una ACCIÓN fuerza la ejecución\n",
    "resultado = doble.collect()\n",
    "resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33fbc0",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Acciones frecuentes sobre RDDs\n",
    "\n",
    "Ejemplos de `take`, `sum`, `mean` y `stdev` para observar resultados inmediatos (acciones).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb54241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "primeros = rdd.take(3)\n",
    "suma = rdd.sum()\n",
    "promedio = rdd.mean()\n",
    "desviacion = rdd.stdev()\n",
    "\n",
    "print(\"Primeros 3 elementos:\", primeros)\n",
    "print(\"Suma total:\", suma)\n",
    "print(\"Promedio:\", promedio)\n",
    "print(\"Desviación estándar:\", round(desviacion, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bea200",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Mini *word count* con limpieza mínima\n",
    "\n",
    "Flujo típico para texto distribuido:\n",
    "1. Separar palabras con `flatMap`.\n",
    "2. Limpiar tokens (solo alfanumérico) y filtrar los muy cortos.\n",
    "3. Contar con `map` y `reduceByKey`.\n",
    "4. Ordenar por frecuencia y mostrar las más comunes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170afab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texto = [\n",
    "    \"Hola mundo desde Spark\",\n",
    "    \"Procesamiento distribuido con PySpark es eficiente\",\n",
    "    \"Hola Spark hola datos\",\n",
    "]\n",
    "\n",
    "rdd_texto = sc.parallelize(texto)\n",
    "palabras = rdd_texto.flatMap(lambda linea: linea.lower().split())\n",
    "\n",
    "# Limpieza: deja solo alfanumérico; descarta tokens cortos\n",
    "limpias = palabras.map(lambda w: ''.join(ch for ch in w if ch.isalnum())) \\\n",
    "                  .filter(lambda w: len(w) >= 3)\n",
    "\n",
    "pares = limpias.map(lambda w: (w, 1))\n",
    "conteo = pares.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "ordenado = conteo.sortBy(lambda kv: kv[1], ascending=False)\n",
    "ordenado.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16760ea4",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Particiones y rendimiento (experimento rápido)\n",
    "\n",
    "El número de particiones influye en el **paralelismo**. Este experimento compara el tiempo de `count()` bajo distintos valores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb17bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "n = 2_000_000\n",
    "rdd_big = sc.parallelize(range(n), numSlices=4)\n",
    "\n",
    "for p in [2, 4, 8, 16]:\n",
    "    t0 = time.time()\n",
    "    _ = rdd_big.repartition(p).count()\n",
    "    dt = time.time() - t0\n",
    "    print(f\"particiones={p:>2} -> tiempo={dt:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b11d8a",
   "metadata": {},
   "source": [
    "\n",
    "## 8) (Opcional) Montar Google Drive\n",
    "\n",
    "Si quieres **guardar** resultados o **leer** archivos desde tu Drive:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# Ejemplo de escritura si montaste el Drive:\n",
    "# conteo.saveAsTextFile('/content/drive/MyDrive/salida_conteo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d74e9e7",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Cierre ordenado de la sesión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6390c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.stop()\n",
    "print(\"Sesión Spark detenida.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}